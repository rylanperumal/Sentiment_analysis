{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> ACML Project- Sentiment Analysis of Twitter Data using Naive Bayes and Support Vector Machine </center>\n",
    "## <center> Rylan Perumal - 1396469 </center>\n",
    "## <center> School of Computer Science and Applied Mathematics </center>\n",
    "<img src=\"../images/wits_logo.png\" alt=\"drawing\" width=\"300\"/>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import re\n",
    "import math\n",
    "import random as rand\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "- Class 1 refers to positive sentiment\n",
    "- Class 2 refers to negative sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading in the pre-processed data\n",
    "tweets = pd.read_csv('data/tweets_pre_processed_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_no_punc</th>\n",
       "      <th>tweet_english</th>\n",
       "      <th>tweet_token</th>\n",
       "      <th>tweet</th>\n",
       "      <th>lang</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Big ol freak just make you wann twerk ðŸ˜‚</td>\n",
       "      <td>Big ol freak just make you wann twerk</td>\n",
       "      <td>Big freak just make you</td>\n",
       "      <td>['big', 'freak', 'just', 'make', 'you']</td>\n",
       "      <td>['big', 'freak', 'make']</td>\n",
       "      <td>en</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The rain thought it was slick ðŸ˜‚ i got my umbre...</td>\n",
       "      <td>The rain thought it was slick i got my umbrell...</td>\n",
       "      <td>The rain thought it was slick i got my umbrell...</td>\n",
       "      <td>['the', 'rain', 'thought', 'it', 'was', 'slick...</td>\n",
       "      <td>['rain', 'thought', 'slick', 'got', 'umbrella'...</td>\n",
       "      <td>en</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @noahcent: I crack myself up dude ðŸ˜‚\\n\\nhttp...</td>\n",
       "      <td>I crack myself up dude</td>\n",
       "      <td>I crack myself up dude</td>\n",
       "      <td>['i', 'crack', 'myself', 'up', 'dude']</td>\n",
       "      <td>['crack', 'dude']</td>\n",
       "      <td>en</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BC I was feeling tf outta myself. ðŸ˜Œ</td>\n",
       "      <td>BC I was feeling tf outta myself</td>\n",
       "      <td>I was feeling myself</td>\n",
       "      <td>['i', 'was', 'feeling', 'myself']</td>\n",
       "      <td>['feeling']</td>\n",
       "      <td>en</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @NBCSPhilly: Feeling ready to pop off. Lite...</td>\n",
       "      <td>Feeling ready to pop off Literally Joel Embiid...</td>\n",
       "      <td>Feeling ready to pop off Literally is really o...</td>\n",
       "      <td>['feeling', 'ready', 'to', 'pop', 'off', 'lite...</td>\n",
       "      <td>['feeling', 'ready', 'pop', 'literally', 'real...</td>\n",
       "      <td>en</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0            Big ol freak just make you wann twerk ðŸ˜‚   \n",
       "1  The rain thought it was slick ðŸ˜‚ i got my umbre...   \n",
       "2  RT @noahcent: I crack myself up dude ðŸ˜‚\\n\\nhttp...   \n",
       "3                BC I was feeling tf outta myself. ðŸ˜Œ   \n",
       "4  RT @NBCSPhilly: Feeling ready to pop off. Lite...   \n",
       "\n",
       "                                       tweet_no_punc  \\\n",
       "0              Big ol freak just make you wann twerk   \n",
       "1  The rain thought it was slick i got my umbrell...   \n",
       "2                             I crack myself up dude   \n",
       "3                   BC I was feeling tf outta myself   \n",
       "4  Feeling ready to pop off Literally Joel Embiid...   \n",
       "\n",
       "                                       tweet_english  \\\n",
       "0                            Big freak just make you   \n",
       "1  The rain thought it was slick i got my umbrell...   \n",
       "2                             I crack myself up dude   \n",
       "3                               I was feeling myself   \n",
       "4  Feeling ready to pop off Literally is really o...   \n",
       "\n",
       "                                         tweet_token  \\\n",
       "0            ['big', 'freak', 'just', 'make', 'you']   \n",
       "1  ['the', 'rain', 'thought', 'it', 'was', 'slick...   \n",
       "2             ['i', 'crack', 'myself', 'up', 'dude']   \n",
       "3                  ['i', 'was', 'feeling', 'myself']   \n",
       "4  ['feeling', 'ready', 'to', 'pop', 'off', 'lite...   \n",
       "\n",
       "                                               tweet lang  class  \n",
       "0                           ['big', 'freak', 'make']   en    1.0  \n",
       "1  ['rain', 'thought', 'slick', 'got', 'umbrella'...   en    1.0  \n",
       "2                                  ['crack', 'dude']   en    1.0  \n",
       "3                                        ['feeling']   en    1.0  \n",
       "4  ['feeling', 'ready', 'pop', 'literally', 'real...   en    1.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(data):\n",
    "    train_data, test_data = train_test_split(data, test_size=0.2)\n",
    "    return test_data, train_data, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_string(text):\n",
    "    text = str(text)\n",
    "    text = text[1:]\n",
    "    text = re.sub(\"]\", \"\", text)\n",
    "    text = re.sub(\"'\", \"\", text)\n",
    "    tokens = re.split(\",\", text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(dataset):\n",
    "    # extracting positive and negative review\n",
    "    positive = dataset.tweet_token[dataset['class'] == 1]    \n",
    "    negative = dataset.tweet_token[dataset['class'] == 0]\n",
    "    \n",
    "    positive_token = positive.apply(lambda x: tokenize_string(x))\n",
    "    negative_token = negative.apply(lambda x: tokenize_string(x))\n",
    "\n",
    "    # creating positive and negative word lists\n",
    "    positive_words = []\n",
    "    negative_words = []\n",
    "    for i in range(len(positive_token)):\n",
    "        array = positive_token.iloc[i]\n",
    "        array = list(set(array))\n",
    "        for j in range(len(array)):\n",
    "            array[j] = array[j].replace(\" \", \"\")\n",
    "        \n",
    "        positive_words.extend(list(set(array)))\n",
    "    for i in range(len(negative_token)):\n",
    "        array = negative_token.iloc[i]\n",
    "        array = list(set(array))\n",
    "        for j in range(len(array)):\n",
    "            array[j] = array[j].replace(\" \", \"\")\n",
    "        \n",
    "        negative_words.extend(list(set(array)))\n",
    "    \n",
    "    # removing whitespace that comes from tokenization\n",
    "    while('' in positive_words):\n",
    "        positive_words.remove('')\n",
    "    while('' in negative_words):\n",
    "        negative_words.remove('')\n",
    "    # extracting the uncommon words\n",
    "    unique_neg = list(set(negative_words) - set(positive_words)) # negative words not in positive words\n",
    "    unique_pos = list(set(positive_words) - set(negative_words)) # positive words not in negative words\n",
    "    # normalisation and increasing likelyness of where uncommon word appears\n",
    "#     positive_words.extend(unique_neg)\n",
    "#     negative_words.extend(unique_pos)\n",
    "    # building word dictionaries, feature extraction\n",
    "    positive_dict = Counter(positive_words)\n",
    "    negative_dict = Counter(negative_words)\n",
    "\n",
    "    return positive_dict, negative_dict, positive_words, negative_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(positive_dict, negative_dict, positive_words, negative_words):\n",
    "    # training \n",
    "    for key in positive_dict:\n",
    "        positive_dict[key] = positive_dict[key] / len(positive_words)\n",
    "    for key in negative_dict:\n",
    "        negative_dict[key] = negative_dict[key] / len(negative_words)\n",
    "    return positive_dict, negative_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes(testset, positive_dict, negative_dict, prior_pos, prior_neg):\n",
    "    # elements of confusion matrix\n",
    "    pos_p = 0 # classified: positive, class: positive\n",
    "    pos_n = 0 # classified: positive, class: negative\n",
    "    neg_n = 0 # classified: negative, class: negative\n",
    "    neg_p = 0 # classified: negative, class: positive\n",
    "    \n",
    "    # iterating through all of the test points\n",
    "    test_token = testset.tweet_token.apply(lambda x: tokenize_string(x))\n",
    "    for i in range(test_token.shape[0]):\n",
    "        tweet = test_token.iloc[i] # a string containg the tweet\n",
    "        for j in range(len(tweet)):\n",
    "            tweet[j] = tweet[j].replace(\" \", \"\")\n",
    "        positive_probability = 1\n",
    "        negative_probability = 1     \n",
    "        for k in tweet:\n",
    "            if k in positive_dict.keys():\n",
    "                positive_probability *= positive_dict[k]\n",
    "            else: # smoothing\n",
    "                positive_probability *= 1 / len(positive_dict)\n",
    "        for k in tweet:\n",
    "            if k in negative_dict.keys():\n",
    "                negative_probability *= negative_dict[k]\n",
    "            else: # smoothing\n",
    "                negative_probability *= 1 / len(negative_dict)\n",
    "        normalisation = positive_probability*prior_pos + negative_probability*prior_neg\n",
    "        posterior_positive = positive_probability*prior_pos/ normalisation\n",
    "        posterior_negative = negative_probability*prior_neg/ normalisation\n",
    "        \n",
    "        if(posterior_positive > posterior_negative):\n",
    "            # tweet has been classified as positive\n",
    "            if(testset.iloc[i]['class'] == 1):\n",
    "                pos_p += 1\n",
    "            else:\n",
    "                pos_n += 1\n",
    "        elif(posterior_positive < posterior_negative):\n",
    "            # tweet has been classified as negative            \n",
    "            if(testset.iloc[i]['class'] == 0):\n",
    "                neg_n += 1\n",
    "            else:\n",
    "                neg_p += 1\n",
    "#         else:\n",
    "#             print(\"Tweet: \", i, \" -> could not classifiy\")\n",
    "    return pos_p, pos_n, neg_p, neg_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(pos_p, pos_n, neg_p, neg_n):\n",
    "    cm = np.array([[pos_p, pos_n], [neg_p, neg_n]])\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Greens)\n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('Prediction')\n",
    "    target_names = ['Positive', 'Negative']\n",
    "    tick_marks = np.arange(len(target_names))\n",
    "    plt.xticks(tick_marks, target_names, rotation=0)\n",
    "    plt.yticks(tick_marks, target_names)\n",
    "    ax.xaxis.set_label_position('top')\n",
    "    ax.xaxis.tick_top()\n",
    "    width, height = cm.shape\n",
    "    for x in range(width):\n",
    "        for y in range(height):\n",
    "            plt.annotate(str(cm[x][y]), xy=(y, x), horizontalalignment='center', verticalalignment='center')\n",
    "    plt.savefig('../nb_confusion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_multiple(tweets, runs=2, training=False, supressed_ouput=False):\n",
    "    accuracy_array = []\n",
    "    precision_t_array = []\n",
    "    precision_n_array = []\n",
    "    precision_array = []\n",
    "    recall_array = []\n",
    "    recall_array_t = []\n",
    "    recall_array_n = []\n",
    "    f1_score_array = []\n",
    "    f1_score_array_t = []    \n",
    "    f1_score_array_n = []\n",
    "    for i in np.arange(1, runs):\n",
    "        # shuffling the dataframe so that we do not get a biased training sample\n",
    "        tweets = tweets.reindex(np.random.permutation(tweets.index))\n",
    "        tweets.reset_index(drop=True)\n",
    "        test_tweets, train_tweets, data = train_test(tweets)\n",
    "        total_tweets = data.shape[0]\n",
    "        # prior probability\n",
    "        prior_pos = (data[data['class'] == 1]).shape[0] / total_tweets\n",
    "        prior_neg = (data[data['class'] == 0]).shape[0] / total_tweets\n",
    "        positive_dict, negative_dict, positive_words, negative_words = feature_extraction(train_tweets)\n",
    "        positive_dict, negative_dict = train(positive_dict, negative_dict, positive_words, negative_words)\n",
    "        if training:\n",
    "             pos_p, pos_n, neg_p, neg_n = naive_bayes(train_tweets, positive_dict, negative_dict, prior_pos, prior_neg) \n",
    "        else:\n",
    "            pos_p, pos_n, neg_p, neg_n = naive_bayes(test_tweets, positive_dict, negative_dict, prior_pos, prior_neg) \n",
    "        if supressed_ouput:\n",
    "            plot_confusion_matrix(pos_p, pos_n, neg_p, neg_n)\n",
    "        \n",
    "        accuracy = (pos_p + neg_n) / (pos_p + pos_n + neg_n + neg_p)\n",
    "        precision_t = pos_p / (pos_p + pos_n)\n",
    "        precision_n = neg_n / (neg_n + neg_p)\n",
    "        recall_t = pos_p / (pos_p + neg_p)\n",
    "        recall_n = neg_n / (neg_n + pos_n)\n",
    "        f1_score_t = 2*((precision_t*recall_t)/(precision_t + recall_t))\n",
    "        f1_score_n = 2*((precision_n*recall_n)/(precision_n + recall_n))\n",
    "        accuracy_array.append(accuracy)\n",
    "        precision_t_array.append(precision_t)\n",
    "        precision_n_array.append(precision_n)\n",
    "        recall_array_t.append(recall_t)\n",
    "        recall_array_n.append(recall_n)\n",
    "        f1_score_array_t.append(f1_score_t)\n",
    "        f1_score_array_n.append(f1_score_n)\n",
    "    precision_array.append(precision_t_array)\n",
    "    precision_array.append(precision_n_array)\n",
    "    recall_array.append(recall_array_t)\n",
    "    recall_array.append(recall_array_n)\n",
    "    f1_score_array.append(f1_score_array_t)\n",
    "    f1_score_array.append(f1_score_array_n)\n",
    "    return accuracy_array, precision_array, recall_array, f1_score_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy = 68.98%\n",
      "Testing Precision positive = 68.81%\n",
      "Testing Precision negative = 69.12%\n",
      "Testing Recall positive = 63.75%\n",
      "Testing Recall negative = 73.74%\n",
      "Testing F1 Score positive = 66.18%\n",
      "Testing F1 Score negative = 71.35%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "accuracy_array_train, precision_array_train, recall_array_train, f1_score_array_train = run_multiple(tweets, training=True)\n",
    "accuracy_array_test, precision_array_test, recall_array_test, f1_score_array_test = run_multiple(tweets, supressed_ouput=True)\n",
    "\n",
    "avg_accuracy_test = sum(accuracy_array_test) / len(accuracy_array_test)\n",
    "avg_precision_test_t = sum(precision_array_test[0]) / len(precision_array_test[0])\n",
    "avg_precision_test_n = sum(precision_array_test[1]) / len(precision_array_test[1])\n",
    "avg_recall_test_t = sum(recall_array_test[0]) / len(recall_array_test[0])\n",
    "avg_recall_test_n = sum(recall_array_test[1]) / len(recall_array_test[1])\n",
    "avg_f1_score_test_t = sum(f1_score_array_test[0]) / len(f1_score_array_test[0])\n",
    "avg_f1_score_test_n = sum(f1_score_array_test[1]) / len(f1_score_array_test[1])\n",
    "\n",
    "avg_accuracy_train = sum(accuracy_array_train) / len(accuracy_array_train)\n",
    "avg_precision_train_t = sum(precision_array_train[0]) / len(precision_array_train[0])\n",
    "avg_precision_train_n = sum(precision_array_train[1]) / len(precision_array_train[1])\n",
    "avg_recall_train_t = sum(recall_array_train[0]) / len(recall_array_train[0])\n",
    "avg_recall_train_n = sum(recall_array_train[1]) / len(recall_array_train[1])\n",
    "avg_f1_score_train_t = sum(f1_score_array_train[0]) / len(f1_score_array_train[0])\n",
    "avg_f1_score_train_n = sum(f1_score_array_train[1]) / len(f1_score_array_train[1])\n",
    "\n",
    "print('Testing Accuracy = {:>2.2f}%'.format(avg_accuracy_test*100))\n",
    "print('Testing Precision positive = {:>2.2f}%'.format(avg_precision_test_t*100))\n",
    "print('Testing Precision negative = {:>2.2f}%'.format(avg_precision_test_n*100))\n",
    "print('Testing Recall positive = {:>2.2f}%'.format(avg_recall_test_t*100))\n",
    "print('Testing Recall negative = {:>2.2f}%'.format(avg_recall_test_n*100))\n",
    "print('Testing F1 Score positive = {:>2.2f}%'.format(avg_f1_score_test_t*100))\n",
    "print('Testing F1 Score negative = {:>2.2f}%'.format(avg_f1_score_test_n*100))\n",
    "print('-------------------------------------------------------------')\n",
    "print('Training Accuracy = {:>2.2f}%'.format(avg_accuracy_train*100))\n",
    "print('Training Precision positive = {:>2.2f}%'.format(avg_precision_train_t*100))\n",
    "print('Training Precision negative = {:>2.2f}%'.format(avg_precision_train_n*100))\n",
    "print('Training Recall positive = {:>2.2f}%'.format(avg_recall_train_t*100))\n",
    "print('Training Recall negative = {:>2.2f}%'.format(avg_recall_train_n*100))\n",
    "print('Training F1 Score positive = {:>2.2f}%'.format(avg_f1_score_train_t*100))\n",
    "print('Training F1 Score negative = {:>2.2f}%'.format(avg_f1_score_train_n*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
