{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> ACML Project- Sentiment Analysis of Twitter Data using Naive Bayes and Support Vector Machine </center>\n",
    "## <center> Rylan Perumal - 1396469 </center>\n",
    "## <center> School of Computer Science and Applied Mathematics </center>\n",
    "<img src=\"../images/wits_logo.png\" alt=\"drawing\" width=\"300\"/>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import re\n",
    "import math\n",
    "import random as rand\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading in the pre-processed data\n",
    "tweets = pd.read_csv('data/tweets_pre_processed_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_no_punc</th>\n",
       "      <th>tweet_english</th>\n",
       "      <th>tweet_token</th>\n",
       "      <th>tweet</th>\n",
       "      <th>lang</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Big ol freak just make you wann twerk ðŸ˜‚</td>\n",
       "      <td>Big ol freak just make you wann twerk</td>\n",
       "      <td>Big freak just make you</td>\n",
       "      <td>['big', 'freak', 'just', 'make', 'you']</td>\n",
       "      <td>['big', 'freak', 'make']</td>\n",
       "      <td>en</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The rain thought it was slick ðŸ˜‚ i got my umbre...</td>\n",
       "      <td>The rain thought it was slick i got my umbrell...</td>\n",
       "      <td>The rain thought it was slick i got my umbrell...</td>\n",
       "      <td>['the', 'rain', 'thought', 'it', 'was', 'slick...</td>\n",
       "      <td>['rain', 'thought', 'slick', 'got', 'umbrella'...</td>\n",
       "      <td>en</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @noahcent: I crack myself up dude ðŸ˜‚\\n\\nhttp...</td>\n",
       "      <td>I crack myself up dude</td>\n",
       "      <td>I crack myself up dude</td>\n",
       "      <td>['i', 'crack', 'myself', 'up', 'dude']</td>\n",
       "      <td>['crack', 'dude']</td>\n",
       "      <td>en</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BC I was feeling tf outta myself. ðŸ˜Œ</td>\n",
       "      <td>BC I was feeling tf outta myself</td>\n",
       "      <td>I was feeling myself</td>\n",
       "      <td>['i', 'was', 'feeling', 'myself']</td>\n",
       "      <td>['feeling']</td>\n",
       "      <td>en</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @NBCSPhilly: Feeling ready to pop off. Lite...</td>\n",
       "      <td>Feeling ready to pop off Literally Joel Embiid...</td>\n",
       "      <td>Feeling ready to pop off Literally is really o...</td>\n",
       "      <td>['feeling', 'ready', 'to', 'pop', 'off', 'lite...</td>\n",
       "      <td>['feeling', 'ready', 'pop', 'literally', 'real...</td>\n",
       "      <td>en</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0            Big ol freak just make you wann twerk ðŸ˜‚   \n",
       "1  The rain thought it was slick ðŸ˜‚ i got my umbre...   \n",
       "2  RT @noahcent: I crack myself up dude ðŸ˜‚\\n\\nhttp...   \n",
       "3                BC I was feeling tf outta myself. ðŸ˜Œ   \n",
       "4  RT @NBCSPhilly: Feeling ready to pop off. Lite...   \n",
       "\n",
       "                                       tweet_no_punc  \\\n",
       "0              Big ol freak just make you wann twerk   \n",
       "1  The rain thought it was slick i got my umbrell...   \n",
       "2                             I crack myself up dude   \n",
       "3                   BC I was feeling tf outta myself   \n",
       "4  Feeling ready to pop off Literally Joel Embiid...   \n",
       "\n",
       "                                       tweet_english  \\\n",
       "0                            Big freak just make you   \n",
       "1  The rain thought it was slick i got my umbrell...   \n",
       "2                             I crack myself up dude   \n",
       "3                               I was feeling myself   \n",
       "4  Feeling ready to pop off Literally is really o...   \n",
       "\n",
       "                                         tweet_token  \\\n",
       "0            ['big', 'freak', 'just', 'make', 'you']   \n",
       "1  ['the', 'rain', 'thought', 'it', 'was', 'slick...   \n",
       "2             ['i', 'crack', 'myself', 'up', 'dude']   \n",
       "3                  ['i', 'was', 'feeling', 'myself']   \n",
       "4  ['feeling', 'ready', 'to', 'pop', 'off', 'lite...   \n",
       "\n",
       "                                               tweet lang  class  \n",
       "0                           ['big', 'freak', 'make']   en    1.0  \n",
       "1  ['rain', 'thought', 'slick', 'got', 'umbrella'...   en    1.0  \n",
       "2                                  ['crack', 'dude']   en    1.0  \n",
       "3                                        ['feeling']   en    1.0  \n",
       "4  ['feeling', 'ready', 'pop', 'literally', 'real...   en    1.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets.tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(data):\n",
    "    train_data, test_data = train_test_split(data, test_size=0.2)\n",
    "    train_data = train_data.reset_index(drop=True)\n",
    "    test_data = test_data.reset_index(drop=True) \n",
    "    train_extraction = train_data.tweet_token.apply(lambda x: tokenize_string(x))\n",
    "    test_extraction = test_data.tweet_token.apply(lambda x: tokenize_string(x))\n",
    "    X_train = []\n",
    "    Y_train = []\n",
    "    X_test = []\n",
    "    Y_test = []\n",
    "    for i in range(train_extraction.shape[0]):\n",
    "        array = train_extraction.iloc[i]\n",
    "        remove_empty = [word for word in array if word == '']\n",
    "        for word in remove_empty: array.remove(word)\n",
    "        remove_word_less_len_3 = [word for word in array if len(word) < 3]\n",
    "        for word in remove_word_less_len_3: array.remove(word)\n",
    "        for j in range(len(array)): \n",
    "            array[j] = array[j].replace(\" \", \"\")\n",
    "        X_train.append(array)\n",
    "        if(train_data['class'].iloc[i] == 0):\n",
    "            Y_train.append(-1)\n",
    "        else:\n",
    "            Y_train.append(train_data['class'].iloc[i])\n",
    "    \n",
    "    for i in range(test_extraction.shape[0]):\n",
    "        array = test_extraction.iloc[i]\n",
    "        remove_empty = [word for word in array if word == '']\n",
    "        for word in remove_empty: array.remove(word)\n",
    "        remove_word_less_len_3 = [word for word in array if len(word) < 3]\n",
    "        for word in remove_word_less_len_3: array.remove(word)\n",
    "        for j in range(len(array)): \n",
    "            array[j] = array[j].replace(\" \", \"\")\n",
    "        X_test.append(array)\n",
    "        if(test_data['class'].iloc[i] == 0):\n",
    "            Y_test.append(-1)\n",
    "        else:\n",
    "            Y_test.append(test_data['class'].iloc[i])\n",
    "    return X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_string(text):\n",
    "    text = str(text)\n",
    "    text = text[1:]\n",
    "#     text = re.sub(\"[\", \"\", text)\n",
    "    text = re.sub(\"]\", \"\", text)\n",
    "    text = re.sub(\"'\", \"\", text)\n",
    "    tokens = re.split(\",\", text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(dataset):\n",
    "    # extracting words to build feature vector\n",
    "    words = []\n",
    "    for i in range(len(dataset)):\n",
    "        words.extend(list(set(dataset[i])))\n",
    "    word_dict = Counter(words)\n",
    "    word_dict = dict.fromkeys(word_dict, 0)\n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm(X, Y, word_dict, alpha=0.95, iterations=1000):\n",
    "    # adding a bias term\n",
    "#     word_dict['b'] = 0\n",
    "    # training using sgd\n",
    "    for i in range(1, iterations):\n",
    "        for j, x in enumerate(X):\n",
    "            val = 0\n",
    "            for k in range(len(x)):\n",
    "                # dot product\n",
    "                if x[k] in word_dict.keys():\n",
    "                    val += word_dict[x[k]]\n",
    "            val *= Y[j]\n",
    "            # misclassification\n",
    "            if(val < 1):\n",
    "                # updating our weights\n",
    "                for k in range(len(x)):\n",
    "                    if x[k] in word_dict.keys():\n",
    "                        word_dict[x[k]] = word_dict[x[k]] + alpha + ((Y[j])) + (-2 * (1/i) * word_dict[x[k]])\n",
    "            else:\n",
    "                # correctly classified\n",
    "                for k in range(len(x)):\n",
    "                    if x[k] in word_dict.keys():\n",
    "                        word_dict[x[k]] = word_dict[x[k]] + alpha + (-2 * (1/i) * word_dict[x[k]])\n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_notes(X, Y, word_dict, lamda=0.95, iterations=500):\n",
    "    # adding a bias term\n",
    "#     word_dict['b'] = 0\n",
    "    # training using sgd\n",
    "    for t in range(1, iterations):\n",
    "        nt = 1 / (lamda*t)\n",
    "        for j, x in enumerate(X):\n",
    "            val = 0\n",
    "            for k in range(len(x)):\n",
    "                # dot product\n",
    "                if x[k] in word_dict.keys():\n",
    "                    val += word_dict[x[k]]\n",
    "            val *= Y[j]\n",
    "            # misclassification\n",
    "            if(val < 1):\n",
    "                # updating our weights\n",
    "                for k in range(len(x)):\n",
    "                    if x[k] in word_dict.keys():\n",
    "                        word_dict[x[k]] = (1- nt*lamda)*word_dict[x[k]] + nt*Y[j]\n",
    "#                         word_dict[x[k]] = word_dict[x[k]] + alpha + ((Y[j])) + (-2 * (1/i) * word_dict[x[k]])\n",
    "            else:\n",
    "                # correctly classified\n",
    "                for k in range(len(x)):\n",
    "                    if x[k] in word_dict.keys():\n",
    "                        word_dict[x[k]] = (1-nt*lamda)*word_dict[x[k]]\n",
    "#                         word_dict[x[k]] = word_dict[x[k]] + alpha + (-2 * (1/i) * word_dict[x[k]])\n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_classification(X, Y, weights):\n",
    "    pos_p = 0\n",
    "    pos_n = 0\n",
    "    neg_p = 0\n",
    "    neg_n = 0\n",
    "    for i, x in enumerate(X):\n",
    "        val = 0\n",
    "#         print(i, x)\n",
    "        for j in range(len(x)):\n",
    "            # dot product\n",
    "            if x[j] in weights.keys():\n",
    "                val += weights[x[j]]\n",
    "            val *= Y[i]\n",
    "        if(val < 1):\n",
    "            # incorrect\n",
    "            if(Y[i] == -1):\n",
    "                pos_n += 1 # predicted positive, actually negative\n",
    "            else:\n",
    "                neg_p += 1 # predictied negative, actually positive\n",
    "        else:\n",
    "            # correct\n",
    "            if(Y[i] == 1):\n",
    "                pos_p += 1 # predicted positive, actually positive\n",
    "            else:\n",
    "                neg_n += 1 # predicted negative, actually negative\n",
    "    return pos_p, pos_n, neg_p, neg_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(pos_p, pos_n, neg_p, neg_n):\n",
    "    cm = np.array([[pos_p, pos_n], [neg_p, neg_n]])\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Reds)\n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('Prediction')\n",
    "    target_names = ['Positive', 'Negative']\n",
    "    tick_marks = np.arange(len(target_names))\n",
    "    plt.xticks(tick_marks, target_names, rotation=0)\n",
    "    plt.yticks(tick_marks, target_names)\n",
    "    ax.xaxis.set_label_position('top')\n",
    "    ax.xaxis.tick_top()\n",
    "    width, height = cm.shape\n",
    "    for x in range(width):\n",
    "        for y in range(height):\n",
    "            plt.annotate(str(cm[x][y]), xy=(y, x), horizontalalignment='center', verticalalignment='center')\n",
    "    plt.savefig('../svm_confusion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_multiple(tweets, runs=2, training=False, supressed_output=False):\n",
    "    accuracy_array = []\n",
    "    precision_t_array = []\n",
    "    precision_n_array = []\n",
    "    precision_array = []\n",
    "    recall_array = []\n",
    "    recall_array_t = []\n",
    "    recall_array_n = []\n",
    "    f1_score_array = []\n",
    "    f1_score_array_t = []    \n",
    "    f1_score_array_n = []\n",
    "    for i in np.arange(1, runs):\n",
    "        # shuffling the dataframe so that we do not get a biased training sample\n",
    "        tweets = tweets.reindex(np.random.permutation(tweets.index))\n",
    "        tweets.reset_index(drop=True)\n",
    "        X_train, Y_train, X_test, Y_test = train_test(tweets)\n",
    "        word_dict = feature_extraction(X_train)\n",
    "#         print('Training ... ')\n",
    "        learned_weights = svm(X_train, Y_train, word_dict)\n",
    "#         print('Training Done ... ')\n",
    "#         print('-----------------------------------------')\n",
    "        if training:\n",
    "            pos_p, pos_n, neg_p, neg_n = svm_classification(X_train, Y_train, learned_weights)\n",
    "        else:\n",
    "            pos_p, pos_n, neg_p, neg_n = svm_classification(X_test, Y_test, learned_weights)\n",
    "        if supressed_output:\n",
    "            plot_confusion_matrix(pos_p, pos_n, neg_p, neg_n)\n",
    "        accuracy = (pos_p + neg_n) / (pos_p + pos_n + neg_n + neg_p)\n",
    "        precision_t = pos_p / (pos_p + pos_n)\n",
    "        precision_n = neg_n / (neg_n + neg_p)\n",
    "        recall_t = pos_p / (pos_p + neg_p)\n",
    "        recall_n = neg_n / (neg_n + pos_n)\n",
    "        f1_score_t = 2*((precision_t*recall_t)/(precision_t + recall_t))\n",
    "        f1_score_n = 2*((precision_n*recall_n)/(precision_n + recall_n))\n",
    "        accuracy_array.append(accuracy)\n",
    "        precision_t_array.append(precision_t)\n",
    "        precision_n_array.append(precision_n)\n",
    "        recall_array_t.append(recall_t)\n",
    "        recall_array_n.append(recall_n)\n",
    "        f1_score_array_t.append(f1_score_t)\n",
    "        f1_score_array_n.append(f1_score_n)\n",
    "    precision_array.append(precision_t_array)\n",
    "    precision_array.append(precision_n_array)\n",
    "    recall_array.append(recall_array_t)\n",
    "    recall_array.append(recall_array_n)\n",
    "    f1_score_array.append(f1_score_array_t)\n",
    "    f1_score_array.append(f1_score_array_n)\n",
    "    return accuracy_array, precision_array, recall_array, f1_score_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy = 60.94%\n",
      "Testing Precision positive = 55.92%\n",
      "Testing Precision negative = 85.20%\n",
      "Testing Recall positive = 94.81%\n",
      "Testing Recall negative = 28.58%\n",
      "Testing F1 Score positive = 70.34%\n",
      "Testing F1 Score negative = 42.80%\n",
      "-------------------------------------------------------------\n",
      "Training Accuracy = 62.08%\n",
      "Training Precision positive = 56.43%\n",
      "Training Precision negative = 88.56%\n",
      "Training Recall positive = 95.86%\n",
      "Training Recall negative = 30.25%\n",
      "Training F1 Score positive = 71.04%\n",
      "Training F1 Score negative = 45.09%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "accuracy_array_train, precision_array_train, recall_array_train, f1_score_array_train = run_multiple(tweets, training=True)\n",
    "accuracy_array_test, precision_array_test, recall_array_test, f1_score_array_test = run_multiple(tweets, supressed_output=True)\n",
    "\n",
    "avg_accuracy_test = sum(accuracy_array_test) / len(accuracy_array_test)\n",
    "avg_precision_test_t = sum(precision_array_test[0]) / len(precision_array_test[0])\n",
    "avg_precision_test_n = sum(precision_array_test[1]) / len(precision_array_test[1])\n",
    "avg_recall_test_t = sum(recall_array_test[0]) / len(recall_array_test[0])\n",
    "avg_recall_test_n = sum(recall_array_test[1]) / len(recall_array_test[1])\n",
    "avg_f1_score_test_t = sum(f1_score_array_test[0]) / len(f1_score_array_test[0])\n",
    "avg_f1_score_test_n = sum(f1_score_array_test[1]) / len(f1_score_array_test[1])\n",
    "\n",
    "avg_accuracy_train = sum(accuracy_array_train) / len(accuracy_array_train)\n",
    "avg_precision_train_t = sum(precision_array_train[0]) / len(precision_array_train[0])\n",
    "avg_precision_train_n = sum(precision_array_train[1]) / len(precision_array_train[1])\n",
    "avg_recall_train_t = sum(recall_array_train[0]) / len(recall_array_train[0])\n",
    "avg_recall_train_n = sum(recall_array_train[1]) / len(recall_array_train[1])\n",
    "avg_f1_score_train_t = sum(f1_score_array_train[0]) / len(f1_score_array_train[0])\n",
    "avg_f1_score_train_n = sum(f1_score_array_train[1]) / len(f1_score_array_train[1])\n",
    "\n",
    "print('Testing Accuracy = {:>2.2f}%'.format(avg_accuracy_test*100))\n",
    "print('Testing Precision positive = {:>2.2f}%'.format(avg_precision_test_t*100))\n",
    "print('Testing Precision negative = {:>2.2f}%'.format(avg_precision_test_n*100))\n",
    "print('Testing Recall positive = {:>2.2f}%'.format(avg_recall_test_t*100))\n",
    "print('Testing Recall negative = {:>2.2f}%'.format(avg_recall_test_n*100))\n",
    "print('Testing F1 Score positive = {:>2.2f}%'.format(avg_f1_score_test_t*100))\n",
    "print('Testing F1 Score negative = {:>2.2f}%'.format(avg_f1_score_test_n*100))\n",
    "print('-------------------------------------------------------------')\n",
    "print('Training Accuracy = {:>2.2f}%'.format(avg_accuracy_train*100))\n",
    "print('Training Precision positive = {:>2.2f}%'.format(avg_precision_train_t*100))\n",
    "print('Training Precision negative = {:>2.2f}%'.format(avg_precision_train_n*100))\n",
    "print('Training Recall positive = {:>2.2f}%'.format(avg_recall_train_t*100))\n",
    "print('Training Recall negative = {:>2.2f}%'.format(avg_recall_train_n*100))\n",
    "print('Training F1 Score positive = {:>2.2f}%'.format(avg_f1_score_train_t*100))\n",
    "print('Training F1 Score negative = {:>2.2f}%'.format(avg_f1_score_train_n*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
